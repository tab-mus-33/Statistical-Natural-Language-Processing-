<h2> Can Hate Be Explained?  </h2>
The project is an excercise in testing the generalizability of State-of-the-art automatic hate speech detection in light of understanding how 'hate' can be explained to a machine learning solution and what components are actually important pivotal for understanding 'hatred'. Is it architecture that's important or is it the ineherent quality of the dataset emanating from lingusitics or socio-linguistics and representations that makes a dataset more "explainable" to a machine agnostic of the architecture? 

<h2>Project abstract </h2> 
Building Automatic Hate Speech detection models is pivotal from the perspective of how
social media platforms have been used to effectively reinforce violence and persecution of minorities in the recent past. However, predictions from such models have largely been found to be biased with poor interpretability and generalization. The aim of this report is to juxtapose the generalization of two pre-trained models (BERT and DistilBERT) trained on the HateXplain dataset. After fine-tuning the model, we will test their generalization performance on a second hate-speech dataset, ETHOS.

<h2>Project Insights</h2> 
Project insights can be found in the pdf of the paper attached, a presentation has also been provided to aid explaination but it is purely optional to read and does not hold any additional insights.

<h2>Project code</h2> 
The project is available at request and has not been uploaded on repo but can be requested for and will be provided for educational purposes in case the requirement is to reproduce the results. 

<h3>Please write to me at Tabish390@gmail.com for the code.</h3>
